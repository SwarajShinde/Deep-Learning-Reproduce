{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/pzSmW2vIog0G7ZXmAeIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwarajShinde/Deep-Learning-Reproduce/blob/master/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpv0TlWWLHEg",
        "colab_type": "text"
      },
      "source": [
        "**Implementing VGG-Net** .\n",
        "\n",
        " The training and validation loop would not be executed.\n",
        "\n",
        "Using Pytorch\n",
        "\n",
        "The Configuration of VGG-16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcS9vbBOM7nu",
        "colab_type": "text"
      },
      "source": [
        "\"\"\"\n",
        "A from scratch implementation of the VGG architecture.\n",
        "\n",
        "Got any questions feel free to Ping :)\n",
        "\n",
        "Programmed by d1nall \n",
        "\n",
        "<swarajshinde20@gmail.com>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGL93cGRKwap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCwCwq8LLY7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG,self).__init__()\n",
        "        self.Conv1 = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
        "        self.Conv2 = nn.Conv2d(in_channels = 64,out_channels = 64,kernel_size = 3,stride = 1,padding=1)\n",
        "        self.Conv3 = nn.Conv2d(in_channels = 64,out_channels = 128,kernel_size = 3,stride = 1,padding=1)\n",
        "        self.Conv4 = nn.Conv2d(in_channels = 128,out_channels = 128,kernel_size = 3,stride = 1,padding=1)\n",
        "        self.Conv5 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv6 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv7 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv8 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv9 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv10 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv11 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv12 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size = 3,stride=1,padding=1)\n",
        "        self.Conv13 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size = 3,stride=1,padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.linear1 = nn.Linear(in_features= 25088, out_features=4096)\n",
        "        self.linear2 = nn.Linear(in_features=4096 , out_features=4096)\n",
        "        self.linear3 = nn.Linear(in_features=4096 , out_features=1000)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "    def forward(self,image):\n",
        "        bs,ch,w,h = image.size()\n",
        "        #print(bs,ch,w,h)\n",
        "        #print(image.size())\n",
        "        x=F.relu(self.Conv1(image))\n",
        "        x=F.relu(self.Conv2(x))\n",
        "        x=self.pool(x)\n",
        "        print(x.size())\n",
        "        x=F.relu(self.Conv3(x))\n",
        "        x=F.relu(self.Conv4(x))\n",
        "        x=self.pool(x)\n",
        "        print(x.size())\n",
        "        x=F.relu(self.Conv5(x))\n",
        "        x=F.relu(self.Conv6(x))\n",
        "        x=F.relu(self.Conv7(x))\n",
        "        x=self.pool(x)\n",
        "        print(x.size())\n",
        "        x=F.relu(self.Conv8(x))\n",
        "        x=F.relu(self.Conv9(x))\n",
        "        x=F.relu(self.Conv10(x))\n",
        "        x=self.pool(x)\n",
        "        print(x.size())\n",
        "        x=F.relu(self.Conv11(x))\n",
        "        x=F.relu(self.Conv12(x))\n",
        "        x=F.relu(self.Conv13(x))\n",
        "        x=self.pool(x)\n",
        "        print(x.size())\n",
        "        x=x.view(bs,-1)\n",
        "        x=F.relu(self.linear1(x))\n",
        "        x=self.drop(x)\n",
        "        print(x.size())\n",
        "        x=F.relu(self.linear2(x))\n",
        "        x=self.drop(x)\n",
        "        print(x.size())\n",
        "        x=F.relu(self.linear3(x))\n",
        "        x=torch.softmax(x,axis=1)\n",
        "        print(x.size())\n",
        "        return x"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1viEgVh8Xs7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4\n",
        "image = torch.rand((batch_size,3,224,224))\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W59nFMa8dDto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG()"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZOY9hWfdy3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "75bf7963-ead5-4c3b-ab68-742d330a2d07"
      },
      "source": [
        "print(model(image))\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 64, 112, 112])\n",
            "torch.Size([4, 128, 56, 56])\n",
            "torch.Size([4, 256, 28, 28])\n",
            "torch.Size([4, 512, 14, 14])\n",
            "torch.Size([4, 512, 7, 7])\n",
            "torch.Size([4, 4096])\n",
            "torch.Size([4, 4096])\n",
            "torch.Size([4, 1000])\n",
            "tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
            "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
            "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
            "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
            "       grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhejrPi7xYXm",
        "colab_type": "text"
      },
      "source": [
        "* The output Tensor has a shape of (batch_size , number of classes)*\n",
        "\n",
        "\n",
        "Every output image size has been printed to verify the dimensions after every Max-Pooling Layers..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J39QVTj1dA9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHYh0N6dCfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "84b8b996-763f-49c7-d6ab-b2d074669ccf"
      },
      "source": [
        "Pytorch_inbuilt_model = models.vgg16()\n",
        "print(Pytorch_inbuilt_model(image).size())\n",
        "print(Pytorch_inbuilt_model(image))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 1000])\n",
            "tensor([[-0.0776, -0.0571,  0.0132,  ..., -0.0124, -0.0736,  0.0116],\n",
            "        [ 0.0248, -0.0433,  0.0271,  ..., -0.0209,  0.0413,  0.0413],\n",
            "        [ 0.0383, -0.0584, -0.0200,  ...,  0.0472, -0.0767, -0.0513],\n",
            "        [-0.0418,  0.0065, -0.0289,  ...,  0.0323, -0.0230, -0.0383]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smCsmYrVyUtk",
        "colab_type": "text"
      },
      "source": [
        "From this we could verify that out implementation of VGG-16 is correct . \n",
        "\n",
        "The Final comparison is done by using pretrained Pytorch-TorchVision Model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDu8EV3IymAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}